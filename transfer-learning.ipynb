{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"},{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Transfer Learning Cifar10\nimport torch,torchvision\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import ToTensor\nimport torch.nn as nn\nfrom torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\ncifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\ncifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-08T04:30:26.095376Z","iopub.execute_input":"2024-03-08T04:30:26.095750Z","iopub.status.idle":"2024-03-08T04:30:27.750175Z","shell.execute_reply.started":"2024-03-08T04:30:26.095720Z","shell.execute_reply":"2024-03-08T04:30:27.749217Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.transforms import Compose, Resize, Normalize","metadata":{"execution":{"iopub.status.busy":"2024-03-08T05:21:10.728414Z","iopub.execute_input":"2024-03-08T05:21:10.728775Z","iopub.status.idle":"2024-03-08T05:21:10.733258Z","shell.execute_reply.started":"2024-03-08T05:21:10.728746Z","shell.execute_reply":"2024-03-08T05:21:10.732375Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"print(len(cifar_trainset))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:30.765048Z","iopub.execute_input":"2024-03-08T04:30:30.765414Z","iopub.status.idle":"2024-03-08T04:30:30.770096Z","shell.execute_reply.started":"2024-03-08T04:30:30.765384Z","shell.execute_reply":"2024-03-08T04:30:30.769089Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"50000\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nif torch.cuda.is_available():\n    device=torch.device(type=\"cuda\", index=0)\nelse:\n    device=torch.device(type=\"cpu\", index=0)   ","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:31.660019Z","iopub.execute_input":"2024-03-08T04:30:31.660783Z","iopub.status.idle":"2024-03-08T04:30:31.665725Z","shell.execute_reply.started":"2024-03-08T04:30:31.660749Z","shell.execute_reply":"2024-03-08T04:30:31.664811Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"batch_size=70\ntrain_dl = DataLoader(cifar_trainset, batch_size=batch_size, shuffle=True)\ntest_dl = DataLoader(cifar_testset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:32.419983Z","iopub.execute_input":"2024-03-08T04:30:32.420373Z","iopub.status.idle":"2024-03-08T04:30:32.425450Z","shell.execute_reply.started":"2024-03-08T04:30:32.420341Z","shell.execute_reply":"2024-03-08T04:30:32.424362Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(len(train_dl))\nprint(len(test_dl))","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:36.015118Z","iopub.execute_input":"2024-03-08T04:30:36.016023Z","iopub.status.idle":"2024-03-08T04:30:36.020678Z","shell.execute_reply.started":"2024-03-08T04:30:36.015993Z","shell.execute_reply":"2024-03-08T04:30:36.019794Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"715\n143\n","output_type":"stream"}]},{"cell_type":"code","source":"random_input=[]\nfor j,(img, labels) in enumerate(train_dl):\n    print(j, img.shape)\n    print(labels)\n    random_input=img\n    random_input2=img[2]\n    break","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:37.184956Z","iopub.execute_input":"2024-03-08T04:30:37.185598Z","iopub.status.idle":"2024-03-08T04:30:37.208383Z","shell.execute_reply.started":"2024-03-08T04:30:37.185563Z","shell.execute_reply":"2024-03-08T04:30:37.207424Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0 torch.Size([70, 3, 32, 32])\ntensor([7, 2, 2, 5, 1, 4, 7, 6, 0, 4, 2, 0, 0, 7, 7, 8, 8, 4, 0, 6, 8, 1, 1, 4,\n        7, 8, 0, 2, 8, 9, 3, 2, 0, 7, 3, 8, 1, 6, 8, 6, 1, 2, 7, 7, 3, 9, 1, 4,\n        5, 0, 2, 9, 1, 0, 3, 8, 0, 0, 0, 5, 6, 5, 7, 5, 2, 0, 9, 3, 6, 0])\n","output_type":"stream"}]},{"cell_type":"code","source":"random_input2.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:38.249283Z","iopub.execute_input":"2024-03-08T04:30:38.249596Z","iopub.status.idle":"2024-03-08T04:30:38.257624Z","shell.execute_reply.started":"2024-03-08T04:30:38.249570Z","shell.execute_reply":"2024-03-08T04:30:38.256658Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 32, 32])"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfor i in range(random_input2.shape[0]):\n    plt.figure()\n    plt.imshow(random_input2[i], cmap='gray')\n    plt.title(f'Channel {i+1}')\n    plt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:41.550011Z","iopub.execute_input":"2024-03-08T04:30:41.550641Z","iopub.status.idle":"2024-03-08T04:30:41.826895Z","shell.execute_reply.started":"2024-03-08T04:30:41.550606Z","shell.execute_reply":"2024-03-08T04:30:41.825973Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom torchvision.models import resnet50, ResNet50_Weights,vgg16\nclass cifar_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pretrainednet=mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT)\n        self.pretrainednet.classifier=nn.Sequential(\n            nn.Linear(in_features=960, out_features=1280, \n                   bias=True),nn.Hardswish(), \n            nn.Dropout(p=0.2, inplace=True), \n            nn.Linear(in_features=1280, out_features=10, \n                      bias=True)\n        )\n    def forward(self, x):\n        x=self.pretrainednet(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:44:54.630135Z","iopub.execute_input":"2024-03-08T04:44:54.630481Z","iopub.status.idle":"2024-03-08T04:44:54.637675Z","shell.execute_reply.started":"2024-03-08T04:44:54.630455Z","shell.execute_reply":"2024-03-08T04:44:54.636733Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nfrom torchvision.models import resnet50, ResNet50_Weights,vgg16\nclass cifar_model_resnet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pretrainednet=resnet50(weights=ResNet50_Weights.DEFAULT)\n        self.classifier=nn.Sequential(\n            nn.Linear(in_features=1000, out_features=1280, \n                   bias=True),nn.Hardswish(), \n            nn.Dropout(p=0.2, inplace=True), \n            nn.Linear(in_features=1280, out_features=10, \n                      bias=True)\n        )\n    def forward(self, x):\n        x=self.pretrainednet(x)\n        x=self.classifier(x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-08T05:55:53.461918Z","iopub.execute_input":"2024-03-08T05:55:53.462624Z","iopub.status.idle":"2024-03-08T05:55:53.470682Z","shell.execute_reply.started":"2024-03-08T05:55:53.462592Z","shell.execute_reply":"2024-03-08T05:55:53.469596Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"model=cifar_model_resnet()\nprint(random_input.shape)\nprint(model(random_input).shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T05:55:54.990854Z","iopub.execute_input":"2024-03-08T05:55:54.991258Z","iopub.status.idle":"2024-03-08T05:55:55.705012Z","shell.execute_reply.started":"2024-03-08T05:55:54.991228Z","shell.execute_reply":"2024-03-08T05:55:55.704004Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"torch.Size([70, 3, 32, 32])\ntorch.Size([70, 10])\n","output_type":"stream"}]},{"cell_type":"code","source":"model=cifar_model()\nprint(random_input.shape)\nprint(model(random_input).shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:30:45.862856Z","iopub.execute_input":"2024-03-08T04:30:45.863250Z","iopub.status.idle":"2024-03-08T04:30:46.177145Z","shell.execute_reply.started":"2024-03-08T04:30:45.863218Z","shell.execute_reply":"2024-03-08T04:30:46.176073Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"torch.Size([70, 3, 32, 32])\ntorch.Size([70, 10])\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_one_epoch(dataloader, model, loss_fn, optimizer):\n    model.train()\n    track_loss=0\n    num_correct=0\n    transform=Compose([Resize((224,224), antialias=True), Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n    for i, (imgs, labels) in enumerate(dataloader):\n        imgs=imgs\n        imgs=transform(imgs)\n        labels=labels\n#         print(\"1\")\n        pred=model(imgs)\n#         print(i)\n        loss=loss_fn(pred, labels)\n        track_loss+=loss.item()\n        num_correct+=(torch.argmax(pred, dim=1)==labels).type(torch.float).sum().item()\n#         print(num_correct)\n        running_loss=round(track_loss/(i+(imgs.shape[0]/batch_size)),2)\n        running_acc=round((num_correct/((i*batch_size+imgs.shape[0])))*100,2)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if i%100==0:\n            print(\"Batch:\", i+1, \"/\",len(dataloader), \"Running Loss:\",running_loss, \"Running Accuracy:\",running_acc)\n            \n    epoch_loss=running_loss\n    epoch_acc=running_acc\n    return round(epoch_loss,2), round(epoch_acc,2)\n\ndef eval_one_epoch(dataloader, model, loss_fn):\n    track_loss=0\n    num_correct=0\n    for i, (imgs, labels) in enumerate(dataloader):\n        pred=model(imgs)\n        loss=loss_fn(pred, labels)\n        track_loss+=loss.item()\n        num_correct+=(torch.argmax(pred, dim=1)==labels).type(torch.float).sum().item()\n#         print(num_correct)\n        \n    epoch_loss=track_loss/len(dataloader)\n    epoch_acc=(num_correct/len(dataloader.dataset))*100\n    return round(epoch_loss,2), round(epoch_acc,2)","metadata":{"execution":{"iopub.status.busy":"2024-03-08T05:22:37.094061Z","iopub.execute_input":"2024-03-08T05:22:37.094790Z","iopub.status.idle":"2024-03-08T05:22:37.106777Z","shell.execute_reply.started":"2024-03-08T05:22:37.094758Z","shell.execute_reply":"2024-03-08T05:22:37.105869Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model=cifar_model()\nmodel=model\n\nloss_fn=nn.CrossEntropyLoss()\nlr=0.001\nfor param in model.pretrainednet.features.parameters():\n    param.requires_grad=False\n# optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\noptimizer=torch.optim.Adam(params=model.parameters(), lr=lr)\nepochs=50\n\nfor i in range(epochs):\n    print(\"Epoch: \", (i+1))\n    train_epoch_loss, train_epoch_acc=train_one_epoch(train_dl, model, loss_fn, optimizer)\n    val_epoch_loss, val_epoch_acc=eval_one_epoch(test_dl, model, loss_fn)\n    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n    print(\"Validation:\", \"Epoch Loss:\", val_epoch_loss, \"Epoch Accuracy:\", val_epoch_acc)\n    print(\"--------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T04:44:58.025059Z","iopub.execute_input":"2024-03-08T04:44:58.025435Z","iopub.status.idle":"2024-03-08T05:10:46.578948Z","shell.execute_reply.started":"2024-03-08T04:44:58.025400Z","shell.execute_reply":"2024-03-08T05:10:46.577239Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch:  1\nTraining: Epoch Loss: 1.79 Epoch Accuracy: 36.52\nValidation: Epoch Loss: 1.74 Epoch Accuracy: 37.65\n--------------------------------------------------\nEpoch:  2\nTraining: Epoch Loss: 1.7 Epoch Accuracy: 39.72\nValidation: Epoch Loss: 1.71 Epoch Accuracy: 39.37\n--------------------------------------------------\nEpoch:  3\nTraining: Epoch Loss: 1.66 Epoch Accuracy: 41.27\nValidation: Epoch Loss: 1.71 Epoch Accuracy: 39.57\n--------------------------------------------------\nEpoch:  4\nTraining: Epoch Loss: 1.63 Epoch Accuracy: 42.78\nValidation: Epoch Loss: 1.73 Epoch Accuracy: 39.82\n--------------------------------------------------\nEpoch:  5\nTraining: Epoch Loss: 1.61 Epoch Accuracy: 43.53\nValidation: Epoch Loss: 1.73 Epoch Accuracy: 39.14\n--------------------------------------------------\nEpoch:  6\nTraining: Epoch Loss: 1.58 Epoch Accuracy: 44.53\nValidation: Epoch Loss: 1.74 Epoch Accuracy: 39.16\n--------------------------------------------------\nEpoch:  7\nTraining: Epoch Loss: 1.56 Epoch Accuracy: 45.56\nValidation: Epoch Loss: 1.73 Epoch Accuracy: 40.35\n--------------------------------------------------\nEpoch:  8\nTraining: Epoch Loss: 1.55 Epoch Accuracy: 45.73\nValidation: Epoch Loss: 1.76 Epoch Accuracy: 39.1\n--------------------------------------------------\nEpoch:  9\nTraining: Epoch Loss: 1.53 Epoch Accuracy: 46.47\nValidation: Epoch Loss: 1.74 Epoch Accuracy: 40.63\n--------------------------------------------------\nEpoch:  10\nTraining: Epoch Loss: 1.52 Epoch Accuracy: 47.28\nValidation: Epoch Loss: 1.78 Epoch Accuracy: 38.44\n--------------------------------------------------\nEpoch:  11\nTraining: Epoch Loss: 1.5 Epoch Accuracy: 47.57\nValidation: Epoch Loss: 1.77 Epoch Accuracy: 39.44\n--------------------------------------------------\nEpoch:  12\nTraining: Epoch Loss: 1.49 Epoch Accuracy: 48.37\nValidation: Epoch Loss: 1.8 Epoch Accuracy: 38.92\n--------------------------------------------------\nEpoch:  13\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m     train_epoch_loss, train_epoch_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     val_epoch_loss, val_epoch_acc\u001b[38;5;241m=\u001b[39meval_one_epoch(test_dl, model, loss_fn)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_epoch_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_epoch_acc)\n","Cell \u001b[0;32mIn[25], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         print(\"1\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         print(i)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss_fn(pred, labels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mcifar_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrainednet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:220\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:210\u001b[0m, in \u001b[0;36mMobileNetV3._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#without transform\nmodel=cifar_model()\nmodel=model\n\nloss_fn=nn.CrossEntropyLoss()\nlr=0.001\nfor param in model.pretrainednet.features.parameters():\n    param.requires_grad=False\n# optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\noptimizer=torch.optim.Adam(params=model.parameters(), lr=lr)\nepochs=50\n\nfor i in range(epochs):\n    print(\"Epoch: \", (i+1))\n    train_epoch_loss, train_epoch_acc=train_one_epoch(train_dl, model, loss_fn, optimizer)\n    val_epoch_loss, val_epoch_acc=eval_one_epoch(test_dl, model, loss_fn)\n    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n    print(\"Validation:\", \"Epoch Loss:\", val_epoch_loss, \"Epoch Accuracy:\", val_epoch_acc)\n    print(\"--------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T05:11:58.438727Z","iopub.execute_input":"2024-03-08T05:11:58.439707Z","iopub.status.idle":"2024-03-08T05:18:41.297791Z","shell.execute_reply.started":"2024-03-08T05:11:58.439673Z","shell.execute_reply":"2024-03-08T05:18:41.296269Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch:  1\nBatch: 1 / 715 Running Loss: 2.32 Running Accuracy: 14.29\nBatch: 101 / 715 Running Loss: 1.95 Running Accuracy: 31.39\nBatch: 201 / 715 Running Loss: 1.88 Running Accuracy: 33.77\nBatch: 301 / 715 Running Loss: 1.85 Running Accuracy: 34.6\nBatch: 401 / 715 Running Loss: 1.83 Running Accuracy: 35.47\nBatch: 501 / 715 Running Loss: 1.81 Running Accuracy: 35.82\nBatch: 601 / 715 Running Loss: 1.8 Running Accuracy: 36.07\nBatch: 701 / 715 Running Loss: 1.79 Running Accuracy: 36.45\nTraining: Epoch Loss: 1.79 Epoch Accuracy: 36.49\nValidation: Epoch Loss: 1.74 Epoch Accuracy: 38.31\n--------------------------------------------------\nEpoch:  2\nBatch: 1 / 715 Running Loss: 1.74 Running Accuracy: 37.14\nBatch: 101 / 715 Running Loss: 1.69 Running Accuracy: 40.37\nBatch: 201 / 715 Running Loss: 1.7 Running Accuracy: 40.04\nBatch: 301 / 715 Running Loss: 1.7 Running Accuracy: 40.07\nBatch: 401 / 715 Running Loss: 1.7 Running Accuracy: 39.95\nBatch: 501 / 715 Running Loss: 1.7 Running Accuracy: 39.98\nBatch: 601 / 715 Running Loss: 1.7 Running Accuracy: 39.89\nBatch: 701 / 715 Running Loss: 1.7 Running Accuracy: 39.87\nTraining: Epoch Loss: 1.7 Epoch Accuracy: 39.89\nValidation: Epoch Loss: 1.73 Epoch Accuracy: 38.9\n--------------------------------------------------\nEpoch:  3\nBatch: 1 / 715 Running Loss: 1.57 Running Accuracy: 41.43\nBatch: 101 / 715 Running Loss: 1.62 Running Accuracy: 42.49\nBatch: 201 / 715 Running Loss: 1.64 Running Accuracy: 41.68\nBatch: 301 / 715 Running Loss: 1.64 Running Accuracy: 41.82\nBatch: 401 / 715 Running Loss: 1.65 Running Accuracy: 41.66\nBatch: 501 / 715 Running Loss: 1.65 Running Accuracy: 41.49\nBatch: 601 / 715 Running Loss: 1.65 Running Accuracy: 41.36\nBatch: 701 / 715 Running Loss: 1.66 Running Accuracy: 41.26\nTraining: Epoch Loss: 1.66 Epoch Accuracy: 41.23\nValidation: Epoch Loss: 1.74 Epoch Accuracy: 38.92\n--------------------------------------------------\nEpoch:  4\nBatch: 1 / 715 Running Loss: 1.51 Running Accuracy: 54.29\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m     train_epoch_loss, train_epoch_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     val_epoch_loss, val_epoch_acc\u001b[38;5;241m=\u001b[39meval_one_epoch(test_dl, model, loss_fn)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_epoch_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_epoch_acc)\n","Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         print(\"1\")\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         print(i)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss_fn(pred, labels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mcifar_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrainednet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:220\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:210\u001b[0m, in \u001b[0;36mMobileNetV3._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#with transform\nmodel=cifar_model()\nmodel=model\n\nloss_fn=nn.CrossEntropyLoss()\nlr=0.001\nfor param in model.pretrainednet.features.parameters():\n    param.requires_grad=False\n# optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\noptimizer=torch.optim.Adam(params=model.parameters(), lr=lr)\nepochs=50\n\nfor i in range(epochs):\n    print(\"Epoch: \", (i+1))\n    train_epoch_loss, train_epoch_acc=train_one_epoch(train_dl, model, loss_fn, optimizer)\n    val_epoch_loss, val_epoch_acc=eval_one_epoch(test_dl, model, loss_fn)\n    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n    print(\"Validation:\", \"Epoch Loss:\", val_epoch_loss, \"Epoch Accuracy:\", val_epoch_acc)\n    print(\"--------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T05:22:42.674852Z","iopub.execute_input":"2024-03-08T05:22:42.675243Z","iopub.status.idle":"2024-03-08T05:52:46.066860Z","shell.execute_reply.started":"2024-03-08T05:22:42.675208Z","shell.execute_reply":"2024-03-08T05:52:46.065385Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch:  1\nBatch: 1 / 715 Running Loss: 2.31 Running Accuracy: 10.0\nBatch: 101 / 715 Running Loss: 1.03 Running Accuracy: 64.67\nBatch: 201 / 715 Running Loss: 0.91 Running Accuracy: 68.3\nBatch: 301 / 715 Running Loss: 0.86 Running Accuracy: 70.06\nBatch: 401 / 715 Running Loss: 0.83 Running Accuracy: 71.08\nBatch: 501 / 715 Running Loss: 0.81 Running Accuracy: 71.76\nBatch: 601 / 715 Running Loss: 0.79 Running Accuracy: 72.4\nBatch: 701 / 715 Running Loss: 0.77 Running Accuracy: 72.96\nTraining: Epoch Loss: 0.77 Epoch Accuracy: 73.01\nValidation: Epoch Loss: 6.52 Epoch Accuracy: 17.05\n--------------------------------------------------\nEpoch:  2\nBatch: 1 / 715 Running Loss: 0.56 Running Accuracy: 78.57\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 15\u001b[0m     train_epoch_loss, train_epoch_acc\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     val_epoch_loss, val_epoch_acc\u001b[38;5;241m=\u001b[39meval_one_epoch(test_dl, model, loss_fn)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_epoch_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_epoch_acc)\n","Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         print(\"1\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         print(i)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss_fn(pred, labels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[27], line 16\u001b[0m, in \u001b[0;36mcifar_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrainednet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:220\u001b[0m, in \u001b[0;36mMobileNetV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:210\u001b[0m, in \u001b[0;36mMobileNetV3._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#with transform\nmodel=cifar_model_resnet()\nmodel=model\n\nloss_fn=nn.CrossEntropyLoss()\nlr=0.001\nmodel.pretrainednet.requires_grad_=False\n# optimizer=torch.optim.SGD(params=model.parameters(), lr=lr)\noptimizer=torch.optim.Adam(params=model.parameters(), lr=lr)\nepochs=50\n\nfor i in range(epochs):\n    print(\"Epoch: \", (i+1))\n    train_epoch_loss, train_epoch_acc=train_one_epoch(train_dl, model, loss_fn, optimizer)\n    val_epoch_loss, val_epoch_acc=eval_one_epoch(test_dl, model, loss_fn)\n    print(\"Training:\", \"Epoch Loss:\", train_epoch_loss, \"Epoch Accuracy:\", train_epoch_acc)\n    print(\"Validation:\", \"Epoch Loss:\", val_epoch_loss, \"Epoch Accuracy:\", val_epoch_acc)\n    print(\"--------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-03-08T07:53:08.898804Z","iopub.execute_input":"2024-03-08T07:53:08.899522Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch:  1\nBatch: 1 / 715 Running Loss: 2.29 Running Accuracy: 8.57\nBatch: 101 / 715 Running Loss: 0.81 Running Accuracy: 73.58\n","output_type":"stream"}]}]}